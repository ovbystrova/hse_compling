{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nefedov_HW7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQbos806hsB20IKYllAqi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ovbystrova/hse_compling/blob/main/hw7/hw7_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-syM-SSNTU17"
      },
      "source": [
        "%%capture\r\n",
        "\r\n",
        "!wget -q --show-progress http://paraphraser.ru/download/get?file_id=1 -O paraphraser.zip\r\n",
        "!unzip paraphraser.zip -d paraphraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982ixPpfVwdG"
      },
      "source": [
        "%%capture\r\n",
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsvkPyPYUzgM"
      },
      "source": [
        "from collections import Counter\r\n",
        "import gensim\r\n",
        "from lxml import html\r\n",
        "import numpy as np\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import pandas as pd\r\n",
        "from pymorphy2 import MorphAnalyzer\r\n",
        "from pymystem3 import Mystem\r\n",
        "from string import punctuation\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# import nltk\r\n",
        "# nltk.download('stopwords')\r\n",
        "m = Mystem()\r\n",
        "morph = MorphAnalyzer()\r\n",
        "punct = punctuation+'«»—…“”*№–'\r\n",
        "stops = set(stopwords.words('russian'))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6japQklUgEM"
      },
      "source": [
        "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\r\n",
        "texts_1 = []\r\n",
        "texts_2 = []\r\n",
        "classes = []\r\n",
        "\r\n",
        "for p in corpus_xml.xpath('//paraphrase'):\r\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\r\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\r\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\r\n",
        "    \r\n",
        "df = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "5IFyQWXdU4bi",
        "outputId": "397a5444-b5dd-473d-df2a-6fbcdaf5d199"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
              "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_1  ... label\n",
              "0  Полицейским разрешат стрелять на поражение по ...  ...     0\n",
              "1  Право полицейских на проникновение в жилище ре...  ...     0\n",
              "2  Президент Египта ввел чрезвычайное положение в...  ...     0\n",
              "3  Вернувшихся из Сирии россиян волнует вопрос тр...  ...    -1\n",
              "4  В Москву из Сирии вернулись 2 самолета МЧС с р...  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU0J3XrcrrJT",
        "outputId": "b32f28ed-4d2d-457d-b638-d56142b3a497"
      },
      "source": [
        "df.label.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     2957\n",
              "-1    2582\n",
              "1     1688\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqE3d3YYVIpp"
      },
      "source": [
        "# Часть 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR4yvL5-VKA9"
      },
      "source": [
        "Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (например вот этой - http://vectors.nlpl.eu/repository/20/180.zip). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше. \r\n",
        "Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие. \r\n",
        "!!!! ВАЖНО: Оценивать модели нужно с помощью кросс-валидации (в семинаре не кросс-валидация)! Метрика - f1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gPpPyRzVmgq"
      },
      "source": [
        "def normalize(text):\r\n",
        "    words = [word.strip(punct) for word in text.lower().split()]\r\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\r\n",
        "    return ' '.join(words)\r\n",
        "\r\n",
        "def get_embedding(text, model, dim):\r\n",
        "    text = text.split()\r\n",
        "    words = Counter(text)\r\n",
        "    total = len(text)\r\n",
        "    vectors = np.zeros((len(words), dim))\r\n",
        "    for i,word in enumerate(words):\r\n",
        "        try:\r\n",
        "            v = model[word]\r\n",
        "            vectors[i] = v*(words[word]/total)\r\n",
        "        except (KeyError, ValueError):\r\n",
        "            continue\r\n",
        "    if vectors.any():\r\n",
        "        vector = np.average(vectors, axis=0)\r\n",
        "    else:\r\n",
        "        vector = np.zeros((dim))\r\n",
        "    return vector"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-f_COF0VdW3"
      },
      "source": [
        "df['text_1_norm'] = df['text_1'].apply(normalize)\r\n",
        "df['text_2_norm'] = df['text_2'].apply(normalize)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpVeHAKlXZ5j"
      },
      "source": [
        "## Word2Vec с нуля (обучается на Википедии)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHcJTwWjY3-s"
      },
      "source": [
        "%%capture\r\n",
        "!wget https://raw.githubusercontent.com/ovbystrova/hse_compling/main/hw3/data/wiki_data.txt -O data/wiki_data.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx7wh3T4YEPL"
      },
      "source": [
        "data = open('data/wiki_data.txt').read().splitlines()[:10000]\r\n",
        "data_norm = [normalize(text) for text in data]\r\n",
        "data_norm = [text for text in data_norm if text]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fonuZt5XWvQA"
      },
      "source": [
        "w2v = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hviFqOZlWgst",
        "outputId": "148a5f38-7c56-4917-c799-bcc4fe39c3b3"
      },
      "source": [
        "dim = 50\r\n",
        "X_text_1_w2v = np.zeros((len(df['text_1_norm']), dim))\r\n",
        "X_text_2_w2v = np.zeros((len(df['text_2_norm']), dim))\r\n",
        "\r\n",
        "for i, text in enumerate(df['text_1_norm'].values):\r\n",
        "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\r\n",
        "    \r\n",
        "for i, text in enumerate(df['text_2_norm'].values):\r\n",
        "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)\r\n",
        "\r\n",
        "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)\r\n",
        "y = df['label'].values"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idG212VLZ8z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e964c20-05c2-401e-dff3-fa6aa6c78b64"
      },
      "source": [
        "# F1-Macro\r\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\r\n",
        "                             class_weight='balanced')\r\n",
        "scores = cross_val_score(clf, X_text_w2v, y, cv=5, scoring='f1_macro')\r\n",
        "print(\"%0.2f f1-score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44 f1-score with a standard deviation of 0.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMbhx0FlaZh4",
        "outputId": "519f5050-15fe-4242-be9a-9679f7e6338a"
      },
      "source": [
        "# F1-Micro\r\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\r\n",
        "                             class_weight='balanced')\r\n",
        "scores = cross_val_score(clf, X_text_w2v, y, cv=5, scoring='f1_micro')\r\n",
        "print(\"%0.2f f1-score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45 f1-score with a standard deviation of 0.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2cU83peanbJ"
      },
      "source": [
        "## Word2Vec предобученный"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzbwzi_ha1yi"
      },
      "source": [
        "%%capture\r\n",
        "!wget -q --show-progress http://vectors.nlpl.eu/repository/20/180.zip -O model.zip\r\n",
        "!unzip model.zip -d model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H72LhOseaqL7"
      },
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('model/model.bin', binary=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrkjc37VbWM9"
      },
      "source": [
        "mapping = {'A': 'ADJ',\r\n",
        "           'ADV': 'ADV',\r\n",
        "           'ADVPRO': 'ADV',\r\n",
        "           'ANUM': 'ADJ',\r\n",
        "           'APRO': 'DET',\r\n",
        "           'COM': 'ADJ',\r\n",
        "           'CONJ': 'SCONJ',\r\n",
        "           'INTJ': 'INTJ',\r\n",
        "           'NONLEX': 'X',\r\n",
        "           'NUM': 'NUM',\r\n",
        "           'PART': 'PART',\r\n",
        "           'PR': 'ADP',\r\n",
        "           'S': 'NOUN',\r\n",
        "           'SPRO': 'PRON',\r\n",
        "           'UNKN': 'X',\r\n",
        "           'V': 'VERB'}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nv8kM6cbd6X"
      },
      "source": [
        "def normalize_mystem(text):\r\n",
        "    tokens = []\r\n",
        "    norm_words = m.analyze(text)\r\n",
        "    for norm_word in norm_words:\r\n",
        "        if 'analysis' not in norm_word:\r\n",
        "            continue\r\n",
        "            \r\n",
        "        if not len(norm_word['analysis']):\r\n",
        "            lemma = norm_word['text']\r\n",
        "            pos = 'UNKN'\r\n",
        "        else:\r\n",
        "            lemma = norm_word[\"analysis\"][0][\"lex\"].lower().strip()\r\n",
        "            pos = norm_word[\"analysis\"][0][\"gr\"].split(',')[0]\r\n",
        "            pos = pos.split('=')[0].strip()\r\n",
        "        pos = mapping[pos]\r\n",
        "        tokens.append(lemma+'_'+pos)\r\n",
        "    return tokens"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GDcYLIgcAxq"
      },
      "source": [
        "df['text_1_norm_m'] = df['text_1'].apply(normalize_mystem)\r\n",
        "df['text_2_norm_m'] = df['text_2'].apply(normalize_mystem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy2GgUXmcB7z"
      },
      "source": [
        "dim = 50\r\n",
        "X_text_1_w2v = np.zeros((len(df['text_1_norm_m']), dim))\r\n",
        "X_text_2_w2v = np.zeros((len(df['text_2_norm_m']), dim))\r\n",
        "\r\n",
        "for i, text in enumerate(df['text_1_norm_m'].values):\r\n",
        "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\r\n",
        "    \r\n",
        "for i, text in enumerate(df['text_2_norm_m'].values):\r\n",
        "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)\r\n",
        "\r\n",
        "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)\r\n",
        "y = df['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EclHoKUcGii"
      },
      "source": [
        "# F1-Macro\r\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\r\n",
        "                             class_weight='balanced')\r\n",
        "scores = cross_val_score(clf, X_text_w2v, y, cv=5, scoring='f1_macro')\r\n",
        "print(\"%0.2f f1-score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYWnRpIrcIwf"
      },
      "source": [
        "# F1-Micro\r\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\r\n",
        "                             class_weight='balanced')\r\n",
        "scores = cross_val_score(clf, X_text_w2v, y, cv=5, scoring='f1_micro')\r\n",
        "print(\"%0.2f f1-score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R9L8enIdoQ7"
      },
      "source": [
        "# Часть 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ0OdYGLdwsh"
      },
      "source": [
        "2) Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-9oU04GeBNb"
      },
      "source": [
        "SVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8b-Qup8ux4x"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK5zD6vKs22e"
      },
      "source": [
        "def tokenize(text):\r\n",
        "    words = [word.strip(punct) for word in text.lower().split()]\r\n",
        "    return ' '.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3K9aELgdpfp"
      },
      "source": [
        "# Word2vec trained on wikipedia\r\n",
        "\r\n",
        "w2v = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)\r\n",
        "dim = 50\r\n",
        "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\r\n",
        "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\r\n",
        "\r\n",
        "for i, text in enumerate(data['text_1_norm'].values):\r\n",
        "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\r\n",
        "    \r\n",
        "for i, text in enumerate(data['text_2_norm'].values):\r\n",
        "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)\r\n",
        "\r\n",
        "w2vec_cosines = cosine_distance(X_text_1_w2v, X_text_2_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKGGW0cqreLg"
      },
      "source": [
        "# Fast-Text trained on wikipedia\r\n",
        "\r\n",
        "fast_text = gensim.models.FastText([text.split() for text in data_norm], size=50, \r\n",
        "                                   min_n=4, max_n=8) \r\n",
        "dim = 50\r\n",
        "data['text_1_notnorm'] = data['text_1'].apply(tokenize)\r\n",
        "data['text_2_notnorm'] = data['text_2'].apply(tokenize)\r\n",
        "\r\n",
        "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\r\n",
        "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\r\n",
        "\r\n",
        "for i, text in enumerate(data['text_1_notnorm'].values):\r\n",
        "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\r\n",
        "    \r\n",
        "for i, text in enumerate(data['text_2_notnorm'].values):\r\n",
        "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)\r\n",
        "\r\n",
        "fasttext_cosines = cosine_distance(X_text_1_w2v, X_text_2_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOewrC4Aq_5A"
      },
      "source": [
        "# SVD\r\n",
        "svd = TruncatedSVD(200)\r\n",
        "\r\n",
        "X_text_1 = svd.fit_transform(tfidf.transform(data['text_1_norm']))\r\n",
        "X_text_2 = svd.fit_transform(tfidf.transform(data['text_2_norm']))\r\n",
        "\r\n",
        "svd_cosines = cosine_distance(X_text_1, X_text_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li51qOgwrDz_"
      },
      "source": [
        "# NMF\r\n",
        "X_text_1_nmf = nmf.transform(tfidf.transform(data['text_1_norm']))\r\n",
        "X_text_2_nmf = nmf.transform(tfidf.transform(data['text_2_norm']))\r\n",
        "\r\n",
        "nmf_cosines = cosine_distance(X_text_1_nmf, X_text_2_nmf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suO1ALhmd3i8"
      },
      "source": [
        "Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).  Попробуйте улучить метрику, изменив параметры в методах векторизации.\r\n",
        "!!УТОЧНЕНИЕ: модель нужно обучить сразу на всех 5 близостях, а не по 1 модели на каждой близости!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7g3dct6d4BJ"
      },
      "source": [
        "cosines = np.concatenate((w2vec_cosines, fasttext_cosines, svd_cosines, nmf_cosines), axis=1)\r\n",
        "cosines.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MthPkgqvtyq"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\r\n",
        "                             class_weight='balanced')\r\n",
        "scores = cross_val_score(clf, cosines, y, cv=5, scoring='f1_micro')\r\n",
        "print(\"%0.2f f1-score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLeO-0EbwCp_"
      },
      "source": [
        "# Перед тем как выполнить следующий код, я подкрутила параметры векторов\r\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\r\n",
        "                             class_weight='balanced')\r\n",
        "scores = cross_val_score(clf, cosines, y, cv=5, scoring='f1_micro')\r\n",
        "print(\"%0.2f f1-score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}